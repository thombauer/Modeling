###@Read data--------
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
options(scipen=99999)
df <- fread("/Users/thombauer/Desktop/classification.csv")
df_status(df)
df <- df %>% mutate(churn=ifelse(success==0,"no churn","churn"))
df <- df[,-4]

set.seed(86)
colnames(df)

###@Remove Redundant Features--------
# ensure the results are repeatable
set.seed(7)
# load the library
library(caret)
# calculate correlation matrix
correlationMatrix <- cor(df[,1:2])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)

###@Rank Features By Importance-------
# ensure results are repeatable
set.seed(7)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model (Y has to be factor, X has to be numeric)
df <- df %>% mutate(churn=ifelse(success==0,"no churn","churn")) %>% mutate(churn=as.factor(churn))
df <- df[,-3]
model <- train(churn~., data=df, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)


###@Feature Selection-------
# ensure the results are repeatable
set.seed(7)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(df[,1:2], df[,3], sizes=c(1:2), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
